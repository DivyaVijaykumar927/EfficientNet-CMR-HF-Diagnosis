{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8396038,"sourceType":"datasetVersion","datasetId":4994834},{"sourceId":8402677,"sourceType":"datasetVersion","datasetId":4999768},{"sourceId":8408124,"sourceType":"datasetVersion","datasetId":5003713},{"sourceId":8606391,"sourceType":"datasetVersion","datasetId":5149831}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, regularizers\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef build_efficientnetb0_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB0 model\n    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Add custom head for classification with L1 and L2 regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l2(0.01),  # L2 regularization\n                     activity_regularizer=regularizers.l1(0.01))(x)  # L1 regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\nTRAIN_PATH = \"/kaggle/input/heart-stages/HEART_TTV/train\"\nVAL_PATH = \"/kaggle/input/heart-stages/HEART_TTV/validation\"\nTEST_PATH = \"/kaggle/input/heart-stages/HEART_TTV/validation\"\n\n\ntraining_file = \"D:\\tra_3.xlsx\"\nvalidation_file = \"D:\\val_3.xlsx\"\ntest_file = \"D:\\test_3.xlsx\"\n\n\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb0_model(input_shape, NUM_CLASSES)\noptimizer = optimizers.RMSprop(learning_rate=1e-4)  # Using RMSprop optimizer\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files\n#save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:24.771650Z","iopub.execute_input":"2024-06-05T14:29:24.772497Z","iopub.status.idle":"2024-06-05T14:29:35.109286Z","shell.execute_reply.started":"2024-06-05T14:29:24.772434Z","shell.execute_reply":"2024-06-05T14:29:35.106659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\npredictions = model.predict(test_ds)\npredicted_classes = np.argmax(predictions, axis=1)\n\n# Get true labels\ntrue_labels = test_ds.classes\n\n# Compute classification report\nclass_names = list(test_ds.class_indices.keys())\nreport = classification_report(true_labels, predicted_classes, target_names=class_names)\n\n# Print classification report\nprint(\"\\nClassification Report:\")\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.109969Z","iopub.status.idle":"2024-06-05T14:29:35.110296Z","shell.execute_reply.started":"2024-06-05T14:29:35.110138Z","shell.execute_reply":"2024-06-05T14:29:35.110152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ndef calculate_classification_report(model, dataset):\n    # Predict classes\n    predictions = model.predict(dataset)\n    y_pred = tf.argmax(predictions, axis=1).numpy()  # Convert predicted probabilities to class labels\n    y_true = dataset.classes\n\n    # Get class labels\n    class_labels = list(dataset.class_indices.keys())\n\n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=class_labels, output_dict=True)\n    return report\n\ntrain_report = calculate_classification_report(model, train_ds)\nval_report = calculate_classification_report(model, val_ds)\ntest_report = calculate_classification_report(model, test_ds)\n\n# Save metrics to Excel files (if needed)\nsave_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n\n# Print classification reports\nprint(\"\\nClassification Report - Train:\")\nprint(pd.DataFrame(train_report).transpose())\nprint(\"\\nClassification Report - Validation:\")\nprint(pd.DataFrame(val_report).transpose())\nprint(\"\\nClassification Report - Test:\")\nprint(pd.DataFrame(test_report).transpose())","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.111279Z","iopub.status.idle":"2024-06-05T14:29:35.111647Z","shell.execute_reply.started":"2024-06-05T14:29:35.111474Z","shell.execute_reply":"2024-06-05T14:29:35.111494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.figure(figsize=(10, 6))\n\n# Plot training accuracy in red\nplt.plot(history.history['accuracy'], label='Train Accuracy', color='red', linewidth=2.5)\n\n# Plot validation accuracy in default blue\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2.5)\n\n# Customize plot appearance\nplt.title('Model Accuracy', fontsize=14, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Accuracy', fontsize=12, fontweight='bold')\nplt.xticks(fontsize=10, fontweight='bold')  # Set font weight of x-axis tick labels to bold\nplt.yticks(fontsize=10, fontweight='bold')  # Set font weight of y-axis tick labels to bold\nplt.grid(False)\n\n# Add legend\nplt.legend(loc='lower right', fontsize=12)\n\n# Show plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.113005Z","iopub.status.idle":"2024-06-05T14:29:35.113327Z","shell.execute_reply.started":"2024-06-05T14:29:35.113176Z","shell.execute_reply":"2024-06-05T14:29:35.113189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training & validation loss values\nplt.figure(figsize=(10, 6))\n\n# Plot training loss in red\nplt.plot(history.history['loss'], label='Train Loss', color='red', linewidth=2.5)\n\n# Plot validation loss in default blue\nplt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2.5)\n\n# Customize plot appearance\nplt.title('Model Loss', fontsize=14, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Loss', fontsize=12, fontweight='bold')\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(False)\n\n# Set font weight of x-axis and y-axis labels to bold\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Loss', fontsize=12, fontweight='bold')\n\n# Add legend\nplt.legend(loc='upper right', fontsize=12)\n\n# Show plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.115335Z","iopub.status.idle":"2024-06-05T14:29:35.115694Z","shell.execute_reply.started":"2024-06-05T14:29:35.115526Z","shell.execute_reply":"2024-06-05T14:29:35.115541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.colors as mcolors\n\ndef plot_confusion_matrix(model, data_generator, dataset_name):\n    # Get true labels from data generator\n    y_true = data_generator.classes\n    \n    # Predict labels\n    y_pred_prob = model.predict(data_generator)\n    y_pred = np.argmax(y_pred_prob, axis=1)\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Get class labels\n    class_labels = list(data_generator.class_indices.keys())\n    \n    # Define custom colormap with softer shades of green\n    cmap = mcolors.ListedColormap(['#C3E6CB', '#A2D9A3', '#7FBF7B', '#5DA55D', '#3C8B3C'])\n    \n    # Plot confusion matrix with custom colormap\n    plt.figure(figsize=(8, 8))\n    \n    # Plot confusion matrix with black lines between cells\n    plt.imshow(cm, interpolation='nearest', cmap=cmap, extent=[-0.5, len(class_labels) - 0.5, -0.5, len(class_labels) - 0.5], origin='upper')\n    \n    # Add title\n    plt.title(f'Confusion Matrix - {dataset_name}', fontsize=14, fontweight='bold')\n    \n    # Add class labels with bold font\n    tick_marks = np.arange(len(class_labels))\n    plt.xticks(tick_marks, class_labels, rotation=45, fontsize=10, ha='right', rotation_mode='anchor', fontweight='bold')\n    plt.yticks(tick_marks, class_labels, fontsize=10, fontweight='bold')\n    \n    # Loop through data and add text annotations (bold)\n    thresh = cm.max() / 2.0\n    for i in range(len(class_labels)):\n        for j in range(len(class_labels)):\n            plt.text(j, i, f'{cm[i, j]}', ha='center', va='center', color='black' if cm[i, j] > thresh else 'black', fontweight='bold')\n    \n    # Set axis labels\n    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n    plt.ylabel('True Label', fontsize=12, fontweight='bold')\n    \n    # Add gridlines between cells\n    for i in range(len(class_labels)):\n        plt.axhline(i - 0.5, color='black', linewidth=1)\n        plt.axvline(i - 0.5, color='black', linewidth=1)\n    \n    # Hide axis ticks\n    plt.tick_params(axis='both', which='both', length=0)\n    \n    # Show plot\n    plt.show()\n\n# Example usage:\n\n# Plot confusion matrix for train dataset\nplot_confusion_matrix(model, train_ds, 'Train')\n\n# Plot confusion matrix for validation dataset\nplot_confusion_matrix(model, val_ds, 'Validation')\n\n# Plot confusion matrix for test dataset\nplot_confusion_matrix(model, test_ds, 'Test')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.117343Z","iopub.status.idle":"2024-06-05T14:29:35.117758Z","shell.execute_reply.started":"2024-06-05T14:29:35.117592Z","shell.execute_reply":"2024-06-05T14:29:35.117606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.colors as mcolors\n\ndef plot_confusion_matrix(model, data_generator, dataset_name):\n    # Get true labels from data generator\n    y_true = data_generator.classes\n    \n    # Predict labels\n    y_pred_prob = model.predict(data_generator)\n    y_pred = np.argmax(y_pred_prob, axis=1)\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Get class labels\n    class_labels = list(data_generator.class_indices.keys())\n    \n    # Define custom colormap with softer shades of green\n    cmap = mcolors.ListedColormap(['#C3E6CB', '#A2D9A3', '#7FBF7B', '#5DA55D', '#3C8B3C'])\n    \n    # Plot confusion matrix with custom colormap\n    plt.figure(figsize=(8, 8))\n    \n    # Plot confusion matrix with black lines between cells\n    plt.imshow(cm, interpolation='nearest', cmap=cmap, extent=[-0.5, len(class_labels) - 0.5, -0.5, len(class_labels) - 0.5], origin='upper')\n    \n    # Add title\n    plt.title(f'Confusion Matrix - {dataset_name}', fontsize=14, fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.118905Z","iopub.status.idle":"2024-06-05T14:29:35.119208Z","shell.execute_reply.started":"2024-06-05T14:29:35.119058Z","shell.execute_reply":"2024-06-05T14:29:35.119072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, regularizers\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.120213Z","iopub.status.idle":"2024-06-05T14:29:35.120552Z","shell.execute_reply.started":"2024-06-05T14:29:35.120366Z","shell.execute_reply":"2024-06-05T14:29:35.120379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_efficientnetb0_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB0 model\n    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Add custom head for classification with L1 and L2 regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l2(0.01),  # L2 regularization\n                     activity_regularizer=regularizers.l1(0.01))(x)  # L1 regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.122614Z","iopub.status.idle":"2024-06-05T14:29:35.123049Z","shell.execute_reply.started":"2024-06-05T14:29:35.122823Z","shell.execute_reply":"2024-06-05T14:29:35.122843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file):\n    # Extract training and validation metrics from history object\n    training_loss = history.history['loss']\n    training_accuracy = history.history['accuracy']\n    validation_loss = history.history['val_loss']\n    validation_accuracy = history.history['val_accuracy']\n\n    # Create DataFrames for training, validation, and test metrics\n    training_metrics_df = pd.DataFrame({\n        'Training Loss': training_loss,\n        'Training Accuracy': training_accuracy\n    })\n    validation_metrics_df = pd.DataFrame({\n        'Validation Loss': validation_loss,\n        'Validation Accuracy': validation_accuracy\n    })\n    test_metrics_df = pd.DataFrame({\n        'Test Loss': [test_loss],\n        'Test Accuracy': [test_accuracy]\n    })\n\n    # Save metrics to specified Excel files\n    training_metrics_df.to_excel(training_file, index=False)\n    validation_metrics_df.to_excel(validation_file, index=False)\n    test_metrics_df.to_excel(test_file, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.124082Z","iopub.status.idle":"2024-06-05T14:29:35.124547Z","shell.execute_reply.started":"2024-06-05T14:29:35.124296Z","shell.execute_reply":"2024-06-05T14:29:35.124316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef build_unet_model(input_shape, num_classes):\n    inputs = tf.keras.Input(shape=input_shape)\n    \n    # Encoder\n    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    # Bottleneck\n    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n\n    # Decoder\n    up4 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv3)\n    up4 = layers.concatenate([up4, conv2], axis=3)\n    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(up4)\n    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv4)\n\n    up5 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv4)\n    up5 = layers.concatenate([up5, conv1], axis=3)\n    conv5 = layers.Conv2D(64, 3, activation='relu', padding='same')(up5)\n    conv5 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv5)\n\n    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    \n    return model\n\n# Define parameters\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\nTRAIN_PATH = \"/path/to/train\"  # Update with your training data directory\nVAL_PATH = \"/path/to/validation\"  # Update with your validation data directory\nTEST_PATH = \"/path/to/test\"  # Update with your test data directory\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='input',  # Set to 'input' for semantic segmentation\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='input',  # Set to 'input' for semantic segmentation\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='input',  # Set to 'input' for semantic segmentation\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the U-Net model\n# Compile the model with sparse categorical cross-entropy loss\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n\n# Train the model\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.126399Z","iopub.status.idle":"2024-06-05T14:29:35.126859Z","shell.execute_reply.started":"2024-06-05T14:29:35.126625Z","shell.execute_reply":"2024-06-05T14:29:35.126644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, regularizers, callbacks\nfrom tensorflow.keras.applications import DenseNet169\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef build_densenet_model(input_shape, num_classes):\n    # Load the pre-trained DenseNet169 model\n    base_model = DenseNet169(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with Dropout and L2 regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.5)(x)  # Add dropout layer\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l2(0.001))(x)  # L2 regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train' # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation' # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test' # Replace with your test data path\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_densenet_model(input_shape, NUM_CLASSES)\n\n# Use Adam optimizer\noptimizer = optimizers.Adam(learning_rate=1e-4)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.129286Z","iopub.status.idle":"2024-06-05T14:29:35.129618Z","shell.execute_reply.started":"2024-06-05T14:29:35.129463Z","shell.execute_reply":"2024-06-05T14:29:35.129476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB7, preprocess_input\nfrom keras.callbacks import ReduceLROnPlateau\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image \nimport keras\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.130727Z","iopub.status.idle":"2024-06-05T14:29:35.131070Z","shell.execute_reply.started":"2024-06-05T14:29:35.130907Z","shell.execute_reply":"2024-06-05T14:29:35.130921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path= '/kaggle/input/heart-31/HEART_3_TTV/train' # Replace with your training data path\nval_path = '/kaggle/input/heart-31/HEART_3_TTV/validation' # Replace with your validation data path\ntest_path = '/kaggle/input/heart-31/HEART_3_TTV/test' # Replace with your test data path\n \ntrain_gen = ImageDataGenerator( horizontal_flip = True,\n                                        rotation_range=30,\n                                        width_shift_range=0.2,\n                                        brightness_range=[0.2,1.2], \n                                       preprocessing_function=preprocess_input )\nval_gen = ImageDataGenerator( preprocessing_function=preprocess_input )\n\n\ntrain_generator = train_gen.flow_from_directory(\n        directory= train_path ,\n        target_size=(224, 224), \n        batch_size=32,\n         shuffle=True,\n        class_mode='categorical',\n       )\n\nvalidation_generator = val_gen.flow_from_directory(\n         directory=val_path ,\n        target_size=(224, 224),  \n        batch_size=32,\n        shuffle=False,\n        class_mode='categorical')\n\nprint(validation_generator.class_indices)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.132599Z","iopub.status.idle":"2024-06-05T14:29:35.132930Z","shell.execute_reply.started":"2024-06-05T14:29:35.132773Z","shell.execute_reply":"2024-06-05T14:29:35.132787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = EfficientNetB7(input_shape = (224,224,3), include_top = False, weights = \"imagenet\",pooling= 'avg' )\n\nfor layers in base_model.layers:\n    layers.trainable = False","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.134276Z","iopub.status.idle":"2024-06-05T14:29:35.134632Z","shell.execute_reply.started":"2024-06-05T14:29:35.134466Z","shell.execute_reply":"2024-06-05T14:29:35.134484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom keras.models import Model\n\n# add new classifier layers\n\nCNN = base_model.output\n#add Dense layer\nCNN = layers.Dense(64, activation='relu')(CNN)\n#add output layer with softmax activation\noutput = layers.Dense(5, activation='softmax')(CNN)\n# define new model\nEfficienNetB7_model = Model(inputs=base_model.inputs, outputs=output)\n# summarize\n#EfficienNetB7_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.136230Z","iopub.status.idle":"2024-06-05T14:29:35.136570Z","shell.execute_reply.started":"2024-06-05T14:29:35.136383Z","shell.execute_reply":"2024-06-05T14:29:35.136395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 2)\nlr_rate = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.1,\n    patience=3, min_lr=0.0001)\n\nEfficienNetB7_model.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-6),  \n    loss=tf.keras.losses.CategoricalCrossentropy(\n    from_logits=False, name='categorical_crossentropy'),\n    metrics=[tf.keras.metrics.CategoricalAccuracy(\n    name='accuracy')])\n\nhistory = EfficienNetB7_model.fit(train_generator, epochs=50, validation_data=validation_generator, callbacks =[callbacks, lr_rate])","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.138014Z","iopub.status.idle":"2024-06-05T14:29:35.138358Z","shell.execute_reply.started":"2024-06-05T14:29:35.138191Z","shell.execute_reply":"2024-06-05T14:29:35.138205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, MaxPooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.regularizers import l2\n\n# Number of classes\nnum_classes = 5\n\n# Load the ResNet50 model pre-trained on ImageNet\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Add custom layers on top of the base model with L2 regularization\nx = base_model.output\nx = GlobalAveragePooling2D()(x)  # Global Average Pooling layer\nx = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)  # Dense layer with ReLU activation and L2 regularization\nx = MaxPooling2D(pool_size=(2, 2))(x)  # Max Pooling layer\npredictions = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(x)  # Output layer with softmax activation and L2 regularization\n\n# Define the model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Freeze the base model layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Data generators for training and validation\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Define the directories for the training, validation, and test datasets\ntrain_dir = '/kaggle/input/heart-stages/HEART_TTV/train'\nval_dir = '/kaggle/input/heart-stages/HEART_TTV/validation'\ntest_dir = '/kaggle/input/heart-stages/HEART_TTV/test'\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\ntest_generator = val_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\n# Custom callback to print metrics after each epoch\nclass PrintMetrics(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        print(f'Epoch {epoch + 1}')\n        print(f'Train Loss: {logs[\"loss\"]:.4f}, Train Accuracy: {logs[\"accuracy\"]:.4f}')\n        print(f'Validation Loss: {logs[\"val_loss\"]:.4f}, Validation Accuracy: {logs[\"val_accuracy\"]:.4f}')\n        print('-' * 30)\n\n# Train the model\nmodel.fit(\n    train_generator,\n    epochs=10,\n    validation_data=val_generator,\n    callbacks=[PrintMetrics()]\n)\n\n# Optionally, unfreeze some of the deeper layers and fine-tune the model\nfor layer in base_model.layers[-50:]:  # Unfreeze the last 50 layers for fine-tuning\n    layer.trainable = True\n\n# Recompile the model with a lower learning rate for fine-tuning\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Continue training with fine-tuning\nmodel.fit(\n    train_generator,\n    epochs=50,\n    validation_data=val_generator,\n    callbacks=[PrintMetrics()]\n)\n\n# Evaluate the model on the test dataset\ntest_loss, test_accuracy = model.evaluate(test_generator)\nprint(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.139954Z","iopub.status.idle":"2024-06-05T14:29:35.140284Z","shell.execute_reply.started":"2024-06-05T14:29:35.140122Z","shell.execute_reply":"2024-06-05T14:29:35.140136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, MaxPooling2D, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.regularizers import l2\n\n# Number of classes\nnum_classes = 5\n\n# Load the ResNet50 model pre-trained on ImageNet\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Add custom layers on top of the base model with L2 regularization\nx = base_model.output\nx = MaxPooling2D(pool_size=(2, 2))(x)  # Max Pooling layer\nx = Flatten()(x)  # Flatten the output before the dense layers\nx = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)  # Dense layer with ReLU activation and L2 regularization\npredictions = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(x)  # Output layer with softmax activation and L2 regularization\n\n# Define the model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Freeze the base model layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Data generators for training and validation\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Define the directories for the training, validation, and test datasets\ntrain_dir = '/kaggle/input/heart-stages/HEART_TTV/train'\nval_dir = '/kaggle/input/heart-stages/HEART_TTV/validation'\ntest_dir = '/kaggle/input/heart-stages/HEART_TTV/test'\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\ntest_generator = val_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\n# Custom callback to print metrics after each epoch\nclass PrintMetrics(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        print(f'Epoch {epoch + 1}')\n        print(f'Train Loss: {logs[\"loss\"]:.4f}, Train Accuracy: {logs[\"accuracy\"]:.4f}')\n        print(f'Validation Loss: {logs[\"val_loss\"]:.4f}, Validation Accuracy: {logs[\"val_accuracy\"]:.4f}')\n        print('-' * 30)\n\n# Train the model\nmodel.fit(\n    train_generator,\n    epochs=10,\n    validation_data=val_generator,\n    callbacks=[PrintMetrics()]\n)\n\n# Optionally, unfreeze some of the deeper layers and fine-tune the model\nfor layer in base_model.layers[-50:]:  # Unfreeze the last 50 layers for fine-tuning\n    layer.trainable = True\n\n# Recompile the model with a lower learning rate for fine-tuning\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Continue training with fine-tuning\nmodel.fit(\n    train_generator,\n    epochs=50,\n    validation_data=val_generator,\n    callbacks=[PrintMetrics()]\n)\n\n# Evaluate the model on the test dataset\ntest_loss, test_accuracy = model.evaluate(test_generator)\nprint(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.141553Z","iopub.status.idle":"2024-06-05T14:29:35.141875Z","shell.execute_reply.started":"2024-06-05T14:29:35.141717Z","shell.execute_reply":"2024-06-05T14:29:35.141731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, MaxPooling2D, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.regularizers import l2\n\n# Number of classes\nnum_classes = 5\n\n# Load the ResNet50 model pre-trained on ImageNet\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Add custom layers on top of the base model with L2 regularization\nx = base_model.output\nx = MaxPooling2D(pool_size=(2, 2))(x)  # Max Pooling layer\nx = Flatten()(x)  # Flatten the output before the dense layers\nx = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(x)  # Dense layer with ReLU activation and L2 regularization\npredictions = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.001))(x)  # Output layer with softmax activation and L2 regularization\n\n# Define the model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Data generators for training and validation\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Define the directories for the training, validation, and test datasets\ntrain_dir = '/kaggle/input/heart-stages/HEART_TTV/train'\nval_dir = '/kaggle/input/heart-stages/HEART_TTV/validation'\ntest_dir = '/kaggle/input/heart-stages/HEART_TTV/test'\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\ntest_generator = val_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\n# Custom callback to print metrics after each epoch\nclass PrintMetrics(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        print(f'Epoch {epoch + 1}')\n        print(f'Train Loss: {logs[\"loss\"]:.4f}, Train Accuracy: {logs[\"accuracy\"]:.4f}')\n        print(f'Validation Loss: {logs[\"val_loss\"]:.4f}, Validation Accuracy: {logs[\"val_accuracy\"]:.4f}')\n        print('-' * 30)\n\n# Train the model\nmodel.fit(\n    train_generator,\n    epochs=50,\n    validation_data=val_generator,\n    callbacks=[PrintMetrics()]\n)\n\n# Recompile the model with a lower learning rate for fine-tuning\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Continue training with fine-tuning\nmodel.fit(\n    train_generator,\n    epochs=50,\n    validation_data=val_generator,\n    callbacks=[PrintMetrics()]\n)\n\n# Evaluate the model on the test dataset\ntest_loss, test_accuracy = model.evaluate(test_generator)\nprint(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:29:35.144786Z","iopub.status.idle":"2024-06-05T14:29:35.145389Z","shell.execute_reply.started":"2024-06-05T14:29:35.145122Z","shell.execute_reply":"2024-06-05T14:29:35.145143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, MaxPooling2D, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.regularizers import l2\n\n# Number of classes\nnum_classes = 5\n\n# Load the ResNet50 model pre-trained on ImageNet\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Add custom layers on top of the base model with L2 regularization\nx = base_model.output\nx = MaxPooling2D(pool_size=(2, 2))(x)  # Max Pooling layer\nx = Flatten()(x)  # Flatten the output before the dense layers\nx = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(x)  # Dense layer with ReLU activation and L2 regularization\npredictions = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.001))(x)  # Output layer with softmax activation and L2 regularization\n\n# Define the model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Data generators for training and validation\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Define the directories for the training, validation, and test datasets\ntrain_dir = '/kaggle/input/heart-31/HEART_3_TTV/train'\nval_dir = '/kaggle/input/heart-31/HEART_3_TTV/validation'\ntest_dir = '/kaggle/input/heart-31/HEART_3_TTV/test'\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\ntest_generator = val_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\n# Custom callback to print metrics after each epoch\nclass PrintMetrics(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        print(f'Epoch {epoch + 1}')\n        print(f'Train Loss: {logs[\"loss\"]:.4f}, Train Accuracy: {logs[\"accuracy\"]:.4f}')\n        print(f'Validation Loss: {logs[\"val_loss\"]:.4f}, Validation Accuracy: {logs[\"val_accuracy\"]:.4f}')\n        print('-' * 30)\n\n# Train the model\nmodel.fit(\n    train_generator,\n    epochs=50,\n    validation_data=val_generator,\n    callbacks=[PrintMetrics()]\n)\n\n# Recompile the model with a lower learning rate for fine-tuning\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Continue training with fine-tuning\nmodel.fit(\n    train_generator,\n    epochs=50,\n    validation_data=val_generator,\n    callbacks=[PrintMetrics()]\n)\n\n# Evaluate the model on the test dataset\ntest_loss, test_accuracy = model.evaluate(test_generator)\nprint(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:40:04.694159Z","iopub.execute_input":"2024-06-05T14:40:04.694993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\n# Define paths to your data directories\ntrain_dir = '/kaggle/input/heart-31/HEART_3_TTV/train'\nvalidation_dir = '/kaggle/input/heart-31/HEART_3_TTV/validation'\ntest_dir = '/kaggle/input/heart-31/HEART_3_TTV/test'\n\n# === New Section for Evaluation and Prediction === #\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T15:51:29.467948Z","iopub.execute_input":"2024-06-05T15:51:29.468773Z","iopub.status.idle":"2024-06-05T15:51:29.474148Z","shell.execute_reply.started":"2024-06-05T15:51:29.468742Z","shell.execute_reply":"2024-06-05T15:51:29.473285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image dimensions and batch size\nimg_width, img_height = 224, 224  # ResNet50 default image size\nbatch_size = 32\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True)\n\nvalid_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = valid_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2024-06-05T15:51:42.486744Z","iopub.execute_input":"2024-06-05T15:51:42.487121Z","iopub.status.idle":"2024-06-05T15:51:42.638682Z","shell.execute_reply.started":"2024-06-05T15:51:42.487087Z","shell.execute_reply":"2024-06-05T15:51:42.637957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the ResNet50 model, pre-trained weights\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n\n# Model architecture\nmodel = Sequential([\n    base_model,\n    GlobalAveragePooling2D(),\n    Dense(1024, activation='relu'),\n    Dense(5, activation='softmax')  # Output layer for 4 classes\n])\n\n# Freeze the layers of the base_model\nfor layer in base_model.layers:\n    layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2024-06-05T15:52:48.612001Z","iopub.execute_input":"2024-06-05T15:52:48.612369Z","iopub.status.idle":"2024-06-05T15:52:49.709273Z","shell.execute_reply.started":"2024-06-05T15:52:48.612341Z","shell.execute_reply":"2024-06-05T15:52:49.708521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-05T15:53:08.536814Z","iopub.execute_input":"2024-06-05T15:53:08.537618Z","iopub.status.idle":"2024-06-05T15:53:08.553668Z","shell.execute_reply.started":"2024-06-05T15:53:08.537585Z","shell.execute_reply":"2024-06-05T15:53:08.552961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model and save the history\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // batch_size,\n    epochs=50,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // batch_size)\n\n# Save the model\nmodel.save('lung_cancer_classification_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-05T15:53:31.245986Z","iopub.execute_input":"2024-06-05T15:53:31.246835Z","iopub.status.idle":"2024-06-05T15:57:48.122196Z","shell.execute_reply.started":"2024-06-05T15:53:31.246803Z","shell.execute_reply":"2024-06-05T15:57:48.120479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, MaxPooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.regularizers import l2\n\n# Number of classes\nnum_classes = 5\n\n# Load the InceptionV3 model pre-trained on ImageNet\nbase_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Add custom layers on top of the base model with L2 regularization\nx = base_model.output\nx = GlobalAveragePooling2D()(x)  # Global Average Pooling layer\nx = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)  # Dense layer with ReLU activation and L2 regularization\nx = MaxPooling2D(pool_size=(2, 2))(x)  # Max Pooling layer\npredictions = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(x)  # Output layer with softmax activation and L2 regularization\n\n# Define the model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Data generators for training and validation\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Define the directories for the training, validation, and test datasets\ntrain_dir = '/kaggle/input/heart-31/HEART_3_TTV/train'\nval_dir = '/kaggle/input/heart-31/HEART_3_TTV/validation'\ntest_dir = '/kaggle/input/heart-31/HEART_3_TTV/test'\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\ntest_generator = val_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\n# Custom callback to print metrics after each epoch\nclass PrintMetrics(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        print(f'Epoch {epoch + 1}')\n        print(f'Train Loss: {logs[\"loss\"]:.4f}, Train Accuracy: {logs[\"accuracy\"]:.4f}')\n        print(f'Validation Loss: {logs[\"val_loss\"]:.4f}, Validation Accuracy: {logs[\"val_accuracy\"]:.4f}')\n        print('-' * 30)\n\n# Train the model\nmodel.fit(\n    train_generator,\n    epochs=50,\n    validation_data=val_generator,\n    callbacks=[PrintMetrics()]\n)\n\n# Evaluate the model on the test dataset\ntest_loss, test_accuracy = model.evaluate(test_generator)\nprint(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.applications import EfficientNetB0\n\ndef build_efficientnetb0_model(input_shape):\n    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=input_shape)\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    return model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:08:46.197350Z","iopub.execute_input":"2024-06-05T17:08:46.198064Z","iopub.status.idle":"2024-06-05T17:08:46.203551Z","shell.execute_reply.started":"2024-06-05T17:08:46.198033Z","shell.execute_reply":"2024-06-05T17:08:46.202207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom skimage.feature import greycomatrix, greycoprops\nfrom skimage.color import rgb2gray\n\ndef extract_glcm_features(image, distances=[5], angles=[0]):\n    gray_image = rgb2gray(image)  # Use rgb2gray from skimage\n    glcm = greycomatrix((gray_image * 255).astype(np.uint8), distances=distances, angles=angles, symmetric=True, normed=True)\n    contrast = greycoprops(glcm, 'contrast').flatten()\n    energy = greycoprops(glcm, 'energy').flatten()\n    homogeneity = greycoprops(glcm, 'homogeneity').flatten()\n    correlation = greycoprops(glcm, 'correlation').flatten()\n    return np.hstack((contrast, energy, homogeneity, correlation))\n\ndef extract_shape_features(image):\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    _, thresh = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    if len(contours) == 0:\n        return np.zeros(8)  # Return zero features if no contours are found\n    cnt = contours[0]\n    area = cv2.contourArea(cnt)\n    perimeter = cv2.arcLength(cnt, True)\n    moments = cv2.moments(cnt)\n    hu_moments = cv2.HuMoments(moments).flatten()\n    return np.hstack((area, perimeter, hu_moments))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:07:49.466582Z","iopub.execute_input":"2024-06-05T17:07:49.466991Z","iopub.status.idle":"2024-06-05T17:07:49.524130Z","shell.execute_reply.started":"2024-06-05T17:07:49.466959Z","shell.execute_reply":"2024-06-05T17:07:49.522949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.applications import EfficientNetB0\n\ndef build_efficientnetb0_model(input_shape):\n    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=input_shape)\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:06:25.687453Z","iopub.execute_input":"2024-06-05T17:06:25.688142Z","iopub.status.idle":"2024-06-05T17:06:40.208308Z","shell.execute_reply.started":"2024-06-05T17:06:25.688111Z","shell.execute_reply":"2024-06-05T17:06:40.207205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########EFFICIENT NET B2 MODEL.......................................................","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, regularizers, callbacks\nfrom tensorflow.keras.applications import EfficientNetB2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef build_efficientnetb2_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB2 model\n    base_model = EfficientNetB2(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with Dropout and L1, L2 regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.5)(x)  # Add dropout layer\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l2(0.01),  # L2 regularization\n                     activity_regularizer=regularizers.l1(0.01))(x)  # L1 regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train' # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation' # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test' # Replace with your test data path\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb2_model(input_shape, NUM_CLASSES)\n\n# Use Adam optimizer\noptimizer = optimizers.Adam(learning_rate=1e-4)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files (you can define this function as needed)\n# save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T05:04:36.401341Z","iopub.execute_input":"2024-06-06T05:04:36.401917Z","iopub.status.idle":"2024-06-06T05:33:20.919007Z","shell.execute_reply.started":"2024-06-06T05:04:36.401889Z","shell.execute_reply":"2024-06-06T05:33:20.917938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_classification_report_and_confusion_matrix(model, dataset, dataset_type):\n    # Predict classes\n    predictions = model.predict(dataset)\n    y_pred = np.argmax(predictions, axis=1)  # Convert predicted probabilities to class labels\n    y_true = dataset.classes\n\n    # Get class labels\n    class_labels = list(dataset.class_indices.keys())\n\n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=class_labels)\n    print(f\"\\n{dataset_type} Classification Report:\")\n    print(report)\n\n    # Generate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plot_confusion_matrix(cm, class_labels, f'Confusion Matrix - {dataset_type}')\n\n    return report\n\ndef plot_confusion_matrix(cm, class_labels, title):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels, cbar=False,\n                annot_kws={\"weight\": \"bold\"})\n    plt.title(title, fontweight='bold')\n    plt.xlabel('PREDICTED LABEL', fontweight='bold')\n    plt.ylabel('TRUE LABEL', fontweight='bold')\n    plt.xticks(fontweight='bold')\n    plt.yticks(fontweight='bold')\n    plt.show()\n\n# Assuming `model`, `train_ds`, `val_ds`, and `test_ds` are already defined elsewhere in your code\n\n# Generate classification reports and confusion matrices\ntrain_report = generate_classification_report_and_confusion_matrix(model, train_ds, 'Training')\nval_report = generate_classification_report_and_confusion_matrix(model, val_ds, 'Validation')\ntest_report = generate_classification_report_and_confusion_matrix(model, test_ds, 'Test')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T05:34:00.222457Z","iopub.execute_input":"2024-06-06T05:34:00.223459Z","iopub.status.idle":"2024-06-06T05:34:55.371888Z","shell.execute_reply.started":"2024-06-06T05:34:00.223422Z","shell.execute_reply":"2024-06-06T05:34:55.370848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('efficientnetB2_721_adam.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T05:35:47.446505Z","iopub.execute_input":"2024-06-06T05:35:47.446906Z","iopub.status.idle":"2024-06-06T05:35:48.297371Z","shell.execute_reply.started":"2024-06-06T05:35:47.446875Z","shell.execute_reply":"2024-06-06T05:35:48.296252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.figure(figsize=(10, 6))\n\n# Plot training accuracy in red\nplt.plot(history.history['accuracy'], label='Train Accuracy', color='red', linewidth=2.5)\n\n# Plot validation accuracy in default blue\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2.5)\n\n# Customize plot appearance\nplt.title('Model Accuracy', fontsize=14, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Accuracy', fontsize=12, fontweight='bold')\nplt.xticks(fontsize=10, fontweight='bold')  # Set font weight of x-axis tick labels to bold\nplt.yticks(fontsize=10, fontweight='bold')  # Set font weight of y-axis tick labels to bold\nplt.grid(False)\n\n# Add legend\nplt.legend(loc='lower right', fontsize=12)\n\n# Show plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T05:36:28.867175Z","iopub.execute_input":"2024-06-06T05:36:28.867517Z","iopub.status.idle":"2024-06-06T05:36:29.209398Z","shell.execute_reply.started":"2024-06-06T05:36:28.867492Z","shell.execute_reply":"2024-06-06T05:36:29.208333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training & validation loss values\nplt.figure(figsize=(10, 6))\n\n# Plot training loss in red\nplt.plot(history.history['loss'], label='Train Loss', color='red', linewidth=2.5)\n\n# Plot validation loss in default blue\nplt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2.5)\n\n# Customize plot appearance\nplt.title('Model Loss', fontsize=14, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Loss', fontsize=12, fontweight='bold')\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(False)\n\n# Set font weight of x-axis and y-axis labels to bold\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Loss', fontsize=12, fontweight='bold')\n\n# Add legend\nplt.legend(loc='upper right', fontsize=12)\n\n# Show plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T05:36:42.286783Z","iopub.execute_input":"2024-06-06T05:36:42.287738Z","iopub.status.idle":"2024-06-06T05:36:42.611144Z","shell.execute_reply.started":"2024-06-06T05:36:42.287697Z","shell.execute_reply":"2024-06-06T05:36:42.610280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###########################################################################################################################################################################\n###EFFICIENT NET B2 WITH ADAMW........................................\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, regularizers, callbacks\nfrom tensorflow.keras.applications import EfficientNetB2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\ndef build_efficientnetb2_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB2 model\n    base_model = EfficientNetB2(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with Dropout and L1, L2 regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.5)(x)  # Add dropout layer\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l2(0.01),  # L2 regularization\n                     activity_regularizer=regularizers.l1(0.01))(x)  # L1 regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train' # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation' # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test' # Replace with your test data path\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb2_model(input_shape, NUM_CLASSES)\n\n# Use ADAMW optimizer\noptimizer = optimizers.AdamW(learning_rate=1e-4)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files (you can define this function as needed)\n# save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T05:44:59.078759Z","iopub.execute_input":"2024-06-06T05:44:59.079103Z","iopub.status.idle":"2024-06-06T06:11:19.493221Z","shell.execute_reply.started":"2024-06-06T05:44:59.079078Z","shell.execute_reply":"2024-06-06T06:11:19.492213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_classification_report_and_confusion_matrix(model, dataset, dataset_type):\n    # Predict classes\n    predictions = model.predict(dataset)\n    y_pred = np.argmax(predictions, axis=1)  # Convert predicted probabilities to class labels\n    y_true = dataset.classes\n\n    # Get class labels\n    class_labels = list(dataset.class_indices.keys())\n\n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=class_labels)\n    print(f\"\\n{dataset_type} Classification Report:\")\n    print(report)\n\n    # Generate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plot_confusion_matrix(cm, class_labels, f'Confusion Matrix - {dataset_type}')\n\n    return report\n\ndef plot_confusion_matrix(cm, class_labels, title):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels, cbar=False,\n                annot_kws={\"weight\": \"bold\"})\n    plt.title(title, fontweight='bold')\n    plt.xlabel('PREDICTED LABEL', fontweight='bold')\n    plt.ylabel('TRUE LABEL', fontweight='bold')\n    plt.xticks(fontweight='bold')\n    plt.yticks(fontweight='bold')\n    plt.show()\n\n# Assuming `model`, `train_ds`, `val_ds`, and `test_ds` are already defined elsewhere in your code\n\n# Generate classification reports and confusion matrices\ntrain_report = generate_classification_report_and_confusion_matrix(model, train_ds, 'Training')\nval_report = generate_classification_report_and_confusion_matrix(model, val_ds, 'Validation')\ntest_report = generate_classification_report_and_confusion_matrix(model, test_ds, 'Test')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:11:43.497033Z","iopub.execute_input":"2024-06-06T06:11:43.497660Z","iopub.status.idle":"2024-06-06T06:12:35.012837Z","shell.execute_reply.started":"2024-06-06T06:11:43.497631Z","shell.execute_reply":"2024-06-06T06:12:35.011870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('effb2_ADAMW.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:13:00.683340Z","iopub.execute_input":"2024-06-06T06:13:00.684042Z","iopub.status.idle":"2024-06-06T06:13:01.373165Z","shell.execute_reply.started":"2024-06-06T06:13:00.684000Z","shell.execute_reply":"2024-06-06T06:13:01.372022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, regularizers, callbacks\nfrom tensorflow.keras.applications import EfficientNetB2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\ndef build_efficientnetb2_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB2 model\n    base_model = EfficientNetB2(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with increased Dropout and Elastic Net regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.4)(x)  # Increase dropout rate to 0.6\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))(x)  # Elastic Net regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train'  # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation'  # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test'  # Replace with your test data path\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb2_model(input_shape, NUM_CLASSES)\n\n# Use Adam optimizer\noptimizer = optimizers.Adam(learning_rate=1e-3)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files (you can define this function as needed)\n# save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:25:01.327691Z","iopub.execute_input":"2024-06-06T06:25:01.328512Z","iopub.status.idle":"2024-06-06T06:49:58.029788Z","shell.execute_reply.started":"2024-06-06T06:25:01.328479Z","shell.execute_reply":"2024-06-06T06:49:58.028823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_classification_report_and_confusion_matrix(model, dataset, dataset_type):\n    # Predict classes\n    predictions = model.predict(dataset)\n    y_pred = np.argmax(predictions, axis=1)  # Convert predicted probabilities to class labels\n    y_true = dataset.classes\n\n    # Get class labels\n    class_labels = list(dataset.class_indices.keys())\n\n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=class_labels)\n    print(f\"\\n{dataset_type} Classification Report:\")\n    print(report)\n\n    # Generate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plot_confusion_matrix(cm, class_labels, f'Confusion Matrix - {dataset_type}')\n\n    return report\n\ndef plot_confusion_matrix(cm, class_labels, title):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels, cbar=False,\n                annot_kws={\"weight\": \"bold\"})\n    plt.title(title, fontweight='bold')\n    plt.xlabel('PREDICTED LABEL', fontweight='bold')\n    plt.ylabel('TRUE LABEL', fontweight='bold')\n    plt.xticks(fontweight='bold')\n    plt.yticks(fontweight='bold')\n    plt.show()\n\n# Assuming `model`, `train_ds`, `val_ds`, and `test_ds` are already defined elsewhere in your code\n\n# Generate classification reports and confusion matrices\ntrain_report = generate_classification_report_and_confusion_matrix(model, train_ds, 'Training')\nval_report = generate_classification_report_and_confusion_matrix(model, val_ds, 'Validation')\ntest_report = generate_classification_report_and_confusion_matrix(model, test_ds, 'Test')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:51:48.958246Z","iopub.execute_input":"2024-06-06T06:51:48.959433Z","iopub.status.idle":"2024-06-06T06:52:39.583497Z","shell.execute_reply.started":"2024-06-06T06:51:48.959385Z","shell.execute_reply":"2024-06-06T06:52:39.582532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('effb2_721_adam2.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T07:00:44.804801Z","iopub.execute_input":"2024-06-06T07:00:44.805168Z","iopub.status.idle":"2024-06-06T07:00:45.653096Z","shell.execute_reply.started":"2024-06-06T07:00:44.805137Z","shell.execute_reply":"2024-06-06T07:00:45.652076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, regularizers, callbacks\nfrom tensorflow.keras.applications import EfficientNetB2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\ndef build_efficientnetb2_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB2 model\n    base_model = EfficientNetB2(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with increased Dropout and Elastic Net regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.4)(x)  # Increase dropout rate to 0.6\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l1_l2(l1=0.1, l2=0.1))(x)  # Elastic Net regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train'  # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation'  # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test'  # Replace with your test data path\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb2_model(input_shape, NUM_CLASSES)\n\n# Use Adam optimizer\noptimizer = optimizers.Adam(learning_rate=1e-3)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files (you can define this function as needed)\n# save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T07:01:09.181289Z","iopub.execute_input":"2024-06-06T07:01:09.181685Z","iopub.status.idle":"2024-06-06T07:27:50.810435Z","shell.execute_reply.started":"2024-06-06T07:01:09.181656Z","shell.execute_reply":"2024-06-06T07:27:50.809498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_classification_report_and_confusion_matrix(model, dataset, dataset_type):\n    # Predict classes\n    predictions = model.predict(dataset)\n    y_pred = np.argmax(predictions, axis=1)  # Convert predicted probabilities to class labels\n    y_true = dataset.classes\n\n    # Get class labels\n    class_labels = list(dataset.class_indices.keys())\n\n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=class_labels)\n    print(f\"\\n{dataset_type} Classification Report:\")\n    print(report)\n\n    # Generate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plot_confusion_matrix(cm, class_labels, f'Confusion Matrix - {dataset_type}')\n\n    return report\n\ndef plot_confusion_matrix(cm, class_labels, title):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels, cbar=False,\n                annot_kws={\"weight\": \"bold\"})\n    plt.title(title, fontweight='bold')\n    plt.xlabel('PREDICTED LABEL', fontweight='bold')\n    plt.ylabel('TRUE LABEL', fontweight='bold')\n    plt.xticks(fontweight='bold')\n    plt.yticks(fontweight='bold')\n    plt.show()\n\n# Assuming `model`, `train_ds`, `val_ds`, and `test_ds` are already defined elsewhere in your code\n\n# Generate classification reports and confusion matrices\ntrain_report = generate_classification_report_and_confusion_matrix(model, train_ds, 'Training')\nval_report = generate_classification_report_and_confusion_matrix(model, val_ds, 'Validation')\ntest_report = generate_classification_report_and_confusion_matrix(model, test_ds, 'Test')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T07:38:17.745202Z","iopub.execute_input":"2024-06-06T07:38:17.745952Z","iopub.status.idle":"2024-06-06T07:39:07.616203Z","shell.execute_reply.started":"2024-06-06T07:38:17.745919Z","shell.execute_reply":"2024-06-06T07:39:07.615293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, regularizers, callbacks\nfrom tensorflow.keras.applications import EfficientNetB2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\ndef build_efficientnetb2_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB2 model\n    base_model = EfficientNetB2(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with increased Dropout and Elastic Net regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.4)(x)  # Increase dropout rate to 0.6\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l1_l2(l1=0.1, l2=0.1))(x)  # Elastic Net regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train'  # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation'  # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test'  # Replace with your test data path\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb2_model(input_shape, NUM_CLASSES)\n\n# Use Adam optimizer\noptimizer = optimizers.Adam(learning_rate=1e-2)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files (you can define this function as needed)\n# save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, regularizers, callbacks\nfrom tensorflow.keras.applications import EfficientNetB2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\ndef build_efficientnetb2_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB2 model\n    base_model = EfficientNetB2(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with increased Dropout and Elastic Net regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.4)(x)  # Increase dropout rate to 0.4\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))(x)  # Elastic Net regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train'  # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation'  # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test'  # Replace with your test data path\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb2_model(input_shape, NUM_CLASSES)\n\n# Use SGD optimizer with momentum\noptimizer = optimizers.SGD(learning_rate=0.01)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files (you can define this function as needed)\n# save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:58:05.155923Z","iopub.execute_input":"2024-06-06T13:58:05.156333Z","iopub.status.idle":"2024-06-06T14:30:10.447779Z","shell.execute_reply.started":"2024-06-06T13:58:05.156301Z","shell.execute_reply":"2024-06-06T14:30:10.446875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_classification_report_and_confusion_matrix(model, dataset, dataset_type):\n    # Predict classes\n    predictions = model.predict(dataset)\n    y_pred = np.argmax(predictions, axis=1)  # Convert predicted probabilities to class labels\n    y_true = dataset.classes\n\n    # Get class labels\n    class_labels = list(dataset.class_indices.keys())\n\n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=class_labels)\n    print(f\"\\n{dataset_type} Classification Report:\")\n    print(report)\n\n    # Generate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plot_confusion_matrix(cm, class_labels, f'Confusion Matrix - {dataset_type}')\n\n    return report\n\ndef plot_confusion_matrix(cm, class_labels, title):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels, cbar=False,\n                annot_kws={\"weight\": \"bold\"})\n    plt.title(title, fontweight='bold')\n    plt.xlabel('PREDICTED LABEL', fontweight='bold')\n    plt.ylabel('TRUE LABEL', fontweight='bold')\n    plt.xticks(fontweight='bold')\n    plt.yticks(fontweight='bold')\n    plt.show()\n\n# Assuming `model`, `train_ds`, `val_ds`, and `test_ds` are already defined elsewhere in your code\n\n# Generate classification reports and confusion matrices\ntrain_report = generate_classification_report_and_confusion_matrix(model, train_ds, 'Training')\nval_report = generate_classification_report_and_confusion_matrix(model, val_ds, 'Validation')\ntest_report = generate_classification_report_and_confusion_matrix(model, test_ds, 'Test')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:30:36.382952Z","iopub.execute_input":"2024-06-06T14:30:36.383926Z","iopub.status.idle":"2024-06-06T14:31:34.487794Z","shell.execute_reply.started":"2024-06-06T14:30:36.383892Z","shell.execute_reply":"2024-06-06T14:31:34.486849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, regularizers, callbacks\nfrom tensorflow.keras.applications import EfficientNetB2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef build_efficientnetb2_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB2 model\n    base_model = EfficientNetB2(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with Dropout and L1, L2 regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.5)(x)  # Add dropout layer\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l2(0.01),  # L2 regularization\n                     activity_regularizer=regularizers.l1(0.01))(x)  # L1 regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train' # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation' # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test' # Replace with your test data path\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb2_model(input_shape, NUM_CLASSES)\n\n# Use Adam optimizer\noptimizer = optimizers.Adam(learning_rate=0.01)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files (you can define this function as needed)\n# save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T15:28:31.224035Z","iopub.execute_input":"2024-06-06T15:28:31.224812Z","iopub.status.idle":"2024-06-06T15:38:25.022119Z","shell.execute_reply.started":"2024-06-06T15:28:31.224783Z","shell.execute_reply":"2024-06-06T15:38:25.020787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, regularizers, callbacks\nfrom tensorflow.keras.applications import EfficientNetB2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\ndef build_efficientnetb2_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB2 model\n    base_model = EfficientNetB2(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with increased Dropout and Elastic Net regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.4)(x)  # Increase dropout rate to 0.4\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))(x)  # Elastic Net regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nEPOCHS = 70\nBATCH_SIZE = 32\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train'  # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation'  # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test'  # Replace with your test data path\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb2_model(input_shape, NUM_CLASSES)\n\n# Use SGD optimizer with momentum\noptimizer = optimizers.SGD(learning_rate=0.01)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files (you can define this function as needed)\n# save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T15:38:37.458580Z","iopub.execute_input":"2024-06-06T15:38:37.458965Z","iopub.status.idle":"2024-06-06T16:20:13.136598Z","shell.execute_reply.started":"2024-06-06T15:38:37.458935Z","shell.execute_reply":"2024-06-06T16:20:13.135733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_classification_report_and_confusion_matrix(model, dataset, dataset_type):\n    # Predict classes\n    predictions = model.predict(dataset)\n    y_pred = np.argmax(predictions, axis=1)  # Convert predicted probabilities to class labels\n    y_true = dataset.classes\n\n    # Get class labels\n    class_labels = list(dataset.class_indices.keys())\n\n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=class_labels)\n    print(f\"\\n{dataset_type} Classification Report:\")\n    print(report)\n\n    # Generate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plot_confusion_matrix(cm, class_labels, f'Confusion Matrix - {dataset_type}')\n\n    return report\n\ndef plot_confusion_matrix(cm, class_labels, title):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels, cbar=False,\n                annot_kws={\"weight\": \"bold\"})\n    plt.title(title, fontweight='bold')\n    plt.xlabel('PREDICTED LABEL', fontweight='bold')\n    plt.ylabel('TRUE LABEL', fontweight='bold')\n    plt.xticks(fontweight='bold')\n    plt.yticks(fontweight='bold')\n    plt.show()\n\n# Assuming `model`, `train_ds`, `val_ds`, and `test_ds` are already defined elsewhere in your code\n\n# Generate classification reports and confusion matrices\ntrain_report = generate_classification_report_and_confusion_matrix(model, train_ds, 'Training')\nval_report = generate_classification_report_and_confusion_matrix(model, val_ds, 'Validation')\ntest_report = generate_classification_report_and_confusion_matrix(model, test_ds, 'Test')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T16:20:38.477786Z","iopub.execute_input":"2024-06-06T16:20:38.478147Z","iopub.status.idle":"2024-06-06T16:21:35.733468Z","shell.execute_reply.started":"2024-06-06T16:20:38.478118Z","shell.execute_reply":"2024-06-06T16:21:35.732593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('efficientnetb2_721_sgd_70.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T16:23:08.331764Z","iopub.execute_input":"2024-06-06T16:23:08.332425Z","iopub.status.idle":"2024-06-06T16:23:08.803239Z","shell.execute_reply.started":"2024-06-06T16:23:08.332377Z","shell.execute_reply":"2024-06-06T16:23:08.802261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom tensorflow.keras.applications import EfficientNetB2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, optimizers, losses, callbacks\n\ndef extract_features(model, generator, num_samples, feature_shape):\n    # Create an empty array to hold the features\n    features = np.zeros(shape=(num_samples, np.prod(feature_shape)))\n    labels = np.zeros(shape=(num_samples, generator.num_classes))\n    \n    i = 0\n    for inputs_batch, labels_batch in generator:\n        batch_size = inputs_batch.shape[0]\n        features_batch = model.predict(inputs_batch)\n        features[i * batch_size: i * batch_size + batch_size] = features_batch.reshape(batch_size, -1)\n        labels[i * batch_size: i * batch_size + batch_size] = labels_batch\n        i += 1\n        if i * batch_size + batch_size >= num_samples:\n            break\n    \n    return features, labels\n\n# Define parameters and paths\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nNUM_CLASSES = 5\nBATCH_SIZE = 32\nEPOCHS = 50\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train'  # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation'  # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test'  # Replace with your test data path\n\n# Data generators\ndatagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Flow validation images in batches from directory\nval_ds = datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Flow test images in batches from directory\ntest_ds = datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Load the pre-trained EfficientNetB2 model\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)\nbase_model = EfficientNetB2(include_top=False, weights='imagenet', input_shape=input_shape)\nbase_model.trainable = False  # Freeze the base model\n\n# Calculate the feature shape\nfeature_shape = base_model.output_shape[1:]\n\n# Extract features\ntrain_features, train_labels = extract_features(base_model, train_ds, train_ds.samples, feature_shape)\nval_features, val_labels = extract_features(base_model, val_ds, val_ds.samples, feature_shape)\ntest_features, test_labels = extract_features(base_model, test_ds, test_ds.samples, feature_shape)\n\n# Flatten the labels\ntrain_labels = np.argmax(train_labels, axis=1)\nval_labels = np.argmax(val_labels, axis=1)\ntest_labels = np.argmax(test_labels, axis=1)\n\n# Train XGBoost model with a specified learning rate\nxgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=NUM_CLASSES, learning_rate=0.01)\nxgb_model.fit(train_features, train_labels, eval_set=[(val_features, val_labels)], early_stopping_rounds=10, verbose=True)\n\n# Evaluate on training dataset\ntrain_predictions = xgb_model.predict(train_features)\ntrain_accuracy = np.mean(train_predictions == train_labels)\n\n# Evaluate on validation dataset\nval_predictions = xgb_model.predict(val_features)\nval_accuracy = np.mean(val_predictions == val_labels)\n\n# Evaluate on test dataset\ntest_predictions = xgb_model.predict(test_features)\ntest_accuracy = np.mean(test_predictions == test_labels)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Accuracy:\", train_accuracy)\nprint(\"Validation Metrics:\")\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, regularizers, callbacks\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report\n\ndef build_efficientnetb3_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB3 model\n    base_model = EfficientNetB3(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with increased Dropout and Elastic Net regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.4)(x)  # Increase dropout rate to 0.4\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))(x)  # Elastic Net regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 300\nIMAGE_HEIGHT = 300\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train'  # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation'  # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test'  \n\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb3_model(input_shape, NUM_CLASSES)\n\n# Use Adam optimizer\noptimizer = optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files (you can define this function as needed)\n# save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T02:32:57.562044Z","iopub.execute_input":"2024-06-07T02:32:57.562828Z","iopub.status.idle":"2024-06-07T03:08:12.169608Z","shell.execute_reply.started":"2024-06-07T02:32:57.562794Z","shell.execute_reply":"2024-06-07T03:08:12.168698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_classification_report_and_confusion_matrix(model, dataset, dataset_type):\n    # Predict classes\n    predictions = model.predict(dataset)\n    y_pred = np.argmax(predictions, axis=1)  # Convert predicted probabilities to class labels\n    y_true = dataset.classes\n\n    # Get class labels\n    class_labels = list(dataset.class_indices.keys())\n\n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=class_labels)\n    print(f\"\\n{dataset_type} Classification Report:\")\n    print(report)\n\n    # Generate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plot_confusion_matrix(cm, class_labels, f'Confusion Matrix - {dataset_type}')\n\n    return report\n\ndef plot_confusion_matrix(cm, class_labels, title):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels, cbar=False,\n                annot_kws={\"weight\": \"bold\"})\n    plt.title(title, fontweight='bold')\n    plt.xlabel('PREDICTED LABEL', fontweight='bold')\n    plt.ylabel('TRUE LABEL', fontweight='bold')\n    plt.xticks(fontweight='bold')\n    plt.yticks(fontweight='bold')\n    plt.show()\n\n# Assuming `model`, `train_ds`, `val_ds`, and `test_ds` are already defined elsewhere in your code\n\n# Generate classification reports and confusion matrices\ntrain_report = generate_classification_report_and_confusion_matrix(model, train_ds, 'Training')\nval_report = generate_classification_report_and_confusion_matrix(model, val_ds, 'Validation')\ntest_report = generate_classification_report_and_confusion_matrix(model, test_ds, 'Test')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T03:08:41.737606Z","iopub.execute_input":"2024-06-07T03:08:41.738317Z","iopub.status.idle":"2024-06-07T03:10:08.013084Z","shell.execute_reply.started":"2024-06-07T03:08:41.738284Z","shell.execute_reply":"2024-06-07T03:10:08.012137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('effB3_adam.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T03:10:40.132257Z","iopub.execute_input":"2024-06-07T03:10:40.132609Z","iopub.status.idle":"2024-06-07T03:10:41.201222Z","shell.execute_reply.started":"2024-06-07T03:10:40.132580Z","shell.execute_reply":"2024-06-07T03:10:41.200381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, regularizers, callbacks\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report\n\ndef build_efficientnetb3_model(input_shape, num_classes):\n    # Load the pre-trained EfficientNetB3 model\n    base_model = EfficientNetB3(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    # Set the base model layers to be trainable (no freezing)\n    base_model.trainable = True\n    \n    # Add custom head for classification with increased Dropout and Elastic Net regularization\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dropout(0.4)(x)  # Increase dropout rate to 0.4\n    x = layers.BatchNormalization()(x)  # Add batch normalization\n    x = layers.Dense(num_classes, \n                     activation='softmax', \n                     kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))(x)  # Elastic Net regularization\n    \n    # Create the model\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    \n    return model\n\n# Define parameters and paths\nIMAGE_WIDTH = 300\nIMAGE_HEIGHT = 300\nNUM_CLASSES = 5\nEPOCHS = 50\nBATCH_SIZE = 32\n\n\nTRAIN_PATH = '/kaggle/input/heart-31/HEART_3_TTV/train'  # Replace with your training data path\nVAL_PATH = '/kaggle/input/heart-31/HEART_3_TTV/validation'  # Replace with your validation data path\nTEST_PATH = '/kaggle/input/heart-31/HEART_3_TTV/test'  \n\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    channel_shift_range=0.1\n)\n\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\n# Flow training images in batches from directory\ntrain_ds = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow validation images in batches from directory\nval_ds = val_datagen.flow_from_directory(\n    VAL_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\n# Flow test images in batches from directory\ntest_ds = test_datagen.flow_from_directory(\n    TEST_PATH,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)  # Input shape for RGB images\n\n# Build the model\nmodel = build_efficientnetb3_model(input_shape, NUM_CLASSES)\n\n# Use RMSprop optimizer\noptimizer = optimizers.RMSprop(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Add callbacks for early stopping and learning rate reduction\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n\n# Train the model with learning rate scheduler and early stopping\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n\n# Evaluate on validation dataset\nval_loss, val_accuracy = model.evaluate(val_ds)\n\n# Evaluate on test dataset\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\n# Save metrics to Excel files (you can define this function as needed)\n# save_metrics_to_excel(history, val_loss, val_accuracy, test_loss, test_accuracy, training_file, validation_file, test_file)\n\n# Print final metrics\nprint(\"Training Metrics:\")\nprint(\"Train Loss:\", history.history['loss'][-1])\nprint(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Validation Metrics:\")\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Test Metrics:\")\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{},"execution_count":null,"outputs":[]}]}